{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ebf91b9",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16332f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a558f4bc",
   "metadata": {},
   "source": [
    "## 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de39b179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 182822/182822 [00:00<00:00, 813730.76 examples/s]\n",
      "Generating test split: 100%|██████████| 6150/6150 [00:00<00:00, 1416138.87 examples/s]\n",
      "Generating validation split: 100%|██████████| 4183/4183 [00:00<00:00, 827193.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"openlifescienceai/medmcqa\")\n",
    "del ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b01d9",
   "metadata": {},
   "source": [
    "### 2.1. Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2fb36eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 182822/182822 [00:00<00:00, 204977.73 examples/s]\n",
      "Map: 100%|██████████| 4183/4183 [00:00<00:00, 173667.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_prompt = \"\"\"Choose the correct option for the following question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Choice:\n",
    "{}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Mapping chỉ số sang nhãn\n",
    "id2label = {\n",
    "    0: 'A',\n",
    "    1: 'B',\n",
    "    2: 'C',\n",
    "    3: 'D'\n",
    "}\n",
    "\n",
    "# Hàm xử lý dữ liệu và tạo prompt\n",
    "def formatting_prompt(examples):\n",
    "    questions = examples[\"question\"]\n",
    "    opas = examples[\"opa\"]\n",
    "    opbs = examples[\"opb\"]\n",
    "    opcs = examples[\"opc\"]\n",
    "    opds = examples[\"opd\"]\n",
    "    cops = examples[\"cop\"]\n",
    "\n",
    "    texts = []\n",
    "    for idx in range(len(questions)):\n",
    "        question = questions[idx]\n",
    "        opa = opas[idx]\n",
    "        opb = opbs[idx]\n",
    "        opc = opcs[idx]\n",
    "        opd = opds[idx]\n",
    "        answer = id2label[cops[idx]]\n",
    "\n",
    "        # Thêm đáp án đúng vào phần trả lời\n",
    "        if answer == \"A\":\n",
    "            answer += \" \" + opa\n",
    "        elif answer == \"B\":\n",
    "            answer += \" \" + opb\n",
    "        elif answer == \"C\":\n",
    "            answer += \" \" + opc\n",
    "        elif answer == \"D\":\n",
    "            answer += \" \" + opd\n",
    "\n",
    "        # Gộp các lựa chọn thành một chuỗi\n",
    "        choices = f\"A. {opa} B. {opb} C. {opc} D. {opd}\"\n",
    "        text = data_prompt.format(question, choices) + answer\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Áp dụng hàm xử lý lên tập dữ liệu\n",
    "process_ds = ds.map(formatting_prompt, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f95b8720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e9ad821a-c438-4965-9f77-760819dfa155',\n",
       " 'question': 'Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma',\n",
       " 'opa': 'Hyperplasia',\n",
       " 'opb': 'Hyperophy',\n",
       " 'opc': 'Atrophy',\n",
       " 'opd': 'Dyplasia',\n",
       " 'cop': 2,\n",
       " 'choice_type': 'single',\n",
       " 'exp': 'Chronic urethral obstruction because of urinary calculi, prostatic hyperophy, tumors, normal pregnancy, tumors, uterine prolapse or functional disorders cause hydronephrosis which by definition is used to describe dilatation of renal pelvis and calculus associated with progressive atrophy of the kidney due to obstruction to the outflow of urine Refer Robbins 7yh/9,1012,9/e. P950',\n",
       " 'subject_name': 'Anatomy',\n",
       " 'topic_name': 'Urinary tract',\n",
       " 'text': 'Choose the correct option for the following question.\\n\\n### Question:\\nChronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma\\n\\n### Choice:\\nA. Hyperplasia B. Hyperophy C. Atrophy D. Dyplasia\\n\\n### Answer:\\nC Atrophy'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268639be",
   "metadata": {},
   "source": [
    "## 3. Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09c0305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.3: Fast Llama patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 15.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.3 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Thiết lập độ dài chuỗi tối đa\n",
    "max_seq_length = 2048\n",
    "\n",
    "# Load mô hình đã nén 4-bit\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")\n",
    "\n",
    "# Thiết lập PEFT với LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\",\n",
    "        \"down_proj\", \"o_proj\", \"gate_proj\"\n",
    "    ],\n",
    "    use_rslora=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# In thông tin các tham số có thể huấn luyện\n",
    "print(model.print_trainable_parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee2b4c",
   "metadata": {},
   "source": [
    "## 4. Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7dec847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"]: 100%|██████████| 182822/182822 [00:04<00:00, 38128.51 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"]: 100%|██████████| 4183/4183 [00:00<00:00, 37564.04 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 182,822 | Num Epochs = 2 | Total steps = 358\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 16 x 1) = 1,024\n",
      " \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='358' max='358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [358/358 3:51:46, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.340500</td>\n",
       "      <td>1.415393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.147400</td>\n",
       "      <td>1.402965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.136500</td>\n",
       "      <td>1.397485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.109000</td>\n",
       "      <td>1.398560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.079500</td>\n",
       "      <td>1.398535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.073600</td>\n",
       "      <td>1.394387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.070200</td>\n",
       "      <td>1.392415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=358, training_loss=1.1351087293145377, metrics={'train_runtime': 13945.5162, 'train_samples_per_second': 26.219, 'train_steps_per_second': 0.026, 'total_flos': 3.670647073422213e+17, 'train_loss': 1.1351087293145377})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thiết lập tham số huấn luyện\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"med-mcqa-llama-3.2-1B-4bit-lora\",\n",
    "    logging_dir=\"logs\",\n",
    "    learning_rate=3e-4,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    per_device_train_batch_size=64,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=2,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "# Khởi tạo trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    train_dataset=process_ds[\"train\"],\n",
    "    eval_dataset=process_ds[\"validation\"],\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "# Bắt đầu huấn luyện\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322cc86b",
   "metadata": {},
   "source": [
    "## 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50ea15af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
      "adapter_model.safetensors: 100%|██████████| 45.1M/45.1M [00:07<00:00, 5.86MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/dainlieu/Llama-3.2-1B-bnb-4bit-MedMCQA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "login(token=hf_token)\n",
    "\n",
    "model.save_pretrained(\"unsloth-llama-trained\")\n",
    "\n",
    "PEFT_MODEL = \"dainlieu/Llama-3.2-1B-bnb-4bit-MedMCQA\"\n",
    "\n",
    "model.push_to_hub(PEFT_MODEL, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952cdbc",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a33948e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.3: Fast Llama patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 15.996 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Output ---\n",
      " Question: What is the capital of France?\n",
      "Choices:\n",
      "A. Berlin\n",
      "B. Paris\n",
      "C. Madrid\n",
      "D. Rome\n",
      "Answer: B\n"
     ]
    }
   ],
   "source": [
    "def infer_from_hf(\n",
    "    model_path=\"dainlieu/Llama-3.2-3B-bnb-4bit-MedMCQA\",\n",
    "    prompt=\"\"\"Question: What is the capital of France?\n",
    "Choices:\n",
    "A. Berlin\n",
    "B. Paris\n",
    "C. Madrid\n",
    "D. Rome\n",
    "Answer:\"\"\"\n",
    "):\n",
    "    # ✅ Load mô hình từ Hugging Face đã fine-tune\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_path,\n",
    "        max_seq_length = 2048,\n",
    "        dtype = None,              # Tự chọn float16/bfloat16\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=False,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\n--- Output ---\\n\", answer)\n",
    "\n",
    "# Gọi hàm\n",
    "infer_from_hf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
